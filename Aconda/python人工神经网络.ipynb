{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf1e7b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Class.py\n",
    "import numpy as np\n",
    "\n",
    "class Option:\n",
    "    def __init__(self):\n",
    "        self.batch_size = 0\n",
    "        self.iteration = 0\n",
    "        \n",
    "class NN:\n",
    "    def __init__(self, **arg):\n",
    "        init = {'layer':[],\n",
    "                'active_function':'sigmoid', \n",
    "                'output_function':'sigmoid', \n",
    "                'learning_rate':1.5, \n",
    "                'weight_decay':0,\n",
    "                'cost':{}, \n",
    "                'encoder':0,\n",
    "                'sparsity':0.03,\n",
    "                'beta':3,\n",
    "                'batch_normalization':0,\n",
    "                'grad_squared':0,\n",
    "                'r':0,\n",
    "                'optimization_method':'normal',\n",
    "                'objective_function':'MSE'\n",
    "               }\n",
    "        \n",
    "        param = dict() #字典结构实现参数列表\n",
    "        param.update(init)\n",
    "        param.update(arg)\n",
    "        \n",
    "        self.size = param['layer'] #取出字典的值初始化网络参数\n",
    "        self.depth = len(self.size)\n",
    "        self.active_function = param['active_function']\n",
    "        self.output_function = param['output_function']\n",
    "        self.learning_rate = param['learning_rate']\n",
    "        self.weight_decay = param['weight_decay']\n",
    "        self.encoder = param['encoder']\n",
    "        self.sparsity = param['sparsity']\n",
    "        self.beta = param['beta']\n",
    "        self.cost = param['cost']\n",
    "        self.batch_normalization = param['batch_normalization']\n",
    "        self.grad_squared = param['grad_squared']\n",
    "        self.r = param['r']\n",
    "        self.optimization_method = param['optimization_method']\n",
    "        self.objective_function = param['objective_function']\n",
    "        self.a = dict()\n",
    "\n",
    "        if self.objective_function == 'Cross Entropy':\n",
    "            self.output_function = 'softmax'\n",
    "\n",
    "        self.W = dict(); self.b = dict(); self.vW = dict(); self.vb = dict() #python必须要先初始化字典才能用\n",
    "        self.rW = dict(); self.rb = dict(); self.sW = dict(); self.sb = dict() #注意要单独初始化，否则它们以后也一直是一样的\n",
    "        self.E = dict(); self.S = dict(); self.Gamma = dict(); self.Beta = dict()\n",
    "        self.vGamma = dict(); self.rGamma = dict(); self.vBeta = dict(); self.rBeta = dict(); \n",
    "        self.sGamma = dict(); self.sBeta = dict(); self.W_grad = dict(); self.b_grad = dict(); self.theta = dict()\n",
    "        self.Gamma_grad = dict(); self.Beta_grad = dict()\n",
    "        \n",
    "        for k in range(self.depth - 1):\n",
    "            width = self.size[k]\n",
    "            height = self.size[k + 1]\n",
    "            #self.W{ k } = (np.random.rand(height, width) - 0.5) * 2 * np.sqrt(6 / (height + width + 1)) - np.sqrt(6 / (height + width + 1));\n",
    "\n",
    "            self.W[k] = 2 * np.random.rand(height, width) / np.sqrt(width) - 1 / np.sqrt(width)\n",
    "            \n",
    "            #self.W{ k } = 2 * np.random.rand(height, width) - 1;\n",
    "            #Xavier initialization\n",
    "            if self.active_function == 'relu':\n",
    "                self.b[k] = np.random.rand(height, 1) + 0.01\n",
    "            else:\n",
    "                self.b[k] = 2 * np.random.rand(height, 1) / np.sqrt(width) - 1 / np.sqrt(width)\n",
    "\n",
    "            #parameters for moments\n",
    "            method = self.optimization_method\n",
    "\n",
    "            if method == 'Momentum':\n",
    "                self.vW[k] = np.zeros((height, width), dtype=float)\n",
    "                self.vb[k] = np.zeros((height, 1), dtype=float)\n",
    "\n",
    "            if method == 'AdaGrad' or method == 'RMSProp' or method == 'Adam':\n",
    "                self.rW[k] = np.zeros((height, width), dtype=float)\n",
    "                self.rb[k] = np.zeros((height, 1), dtype=float)\n",
    "\n",
    "            if method == 'Adam':\n",
    "                self.sW[k] = np.zeros((height, width), dtype=float)\n",
    "                self.sb[k] = np.zeros((height, 1), dtype=float)\n",
    "\n",
    "            #parameters for batch normalization.\n",
    "            if self.batch_normalization:\n",
    "                self.E[k] = np.zeros((height, 1), dtype=float)\n",
    "                self.S[k] = np.zeros((height, 1), dtype=float)\n",
    "                self.Gamma[k] = 1\n",
    "                self.Beta[k] = 0\n",
    "\n",
    "                if  method == 'Momentum':\n",
    "                    self.vGamma[k] = 1\n",
    "                    self.vBeta[k] = 0\n",
    "\n",
    "                if method == 'AdaGrad' or method == 'RMSProp' or method == 'Adam':\n",
    "                    self.rW[k] = np.zeros((height, width), dtype=float)\n",
    "                    self.rb[k] = np.zeros((height, 1), dtype=float)\n",
    "                    self.rGamma[k] = 0\n",
    "                    self.rBeta[k] = 0\n",
    "\n",
    "                if  method == 'Adam':\n",
    "                    self.sGamma[k] = 1\n",
    "                    self.sBeta[k] = 0\n",
    "\n",
    "                self.vecNum = 0\n",
    "                \n",
    "            self.W_grad[k] = np.zeros((height, width), dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "202dae4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function.py\n",
    "np.seterr(divide='ignore', invalid='ignore')\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"返回对应的概率值\"\"\"\n",
    "    exp_x = np.exp(x)\n",
    "    softmax_x = np.zeros(x.shape,dtype=float)\n",
    "    for i in range(len(x[0])):\n",
    "        softmax_x[:,i] = exp_x[:,i] / (exp_x[0,i] + exp_x[1,i])\n",
    "        \n",
    "    return softmax_x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7baa6b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nn_applygradient.py\n",
    "\n",
    "def nn_applygradient(nn):\n",
    "    method = nn.optimization_method\n",
    "    if method == 'AdaGrad' or method == 'RMSProp' or method == 'Adam':\n",
    "        grad_squared = 0\n",
    "        if nn.batch_normalization == 0:\n",
    "            for k in range(nn.depth-1):\n",
    "                grad_squared = grad_squared + sum(sum(nn.W_grad[k]**2)) + sum(nn.b_grad[k]**2)\n",
    "        else:\n",
    "            for k in range(nn.depth-1):\n",
    "                grad_squared = grad_squared + sum(sum(nn.W_grad[k]**2)) + sum(nn.b_grad[k]**2) + nn.Gamma[k]**2 + nn.Beta[k]**2\n",
    "\n",
    "    for k in range(nn.depth-1):\n",
    "        if nn.batch_normalization == 0:\n",
    "            if method == 'normal':\n",
    "                nn.W[k] = nn.W[k] - nn.learning_rate*nn.W_grad[k]\n",
    "                nn.b[k] = nn.b[k] - nn.learning_rate*nn.b_grad[k]\n",
    "                \n",
    "            elif method == 'AdaGrad':\n",
    "                nn.rW[k] = nn.rW[k] + nn.W_grad[k]**2\n",
    "                nn.rb[k] = nn.rb[k] + nn.b_grad[k]**2\n",
    "                nn.W[k] = nn.W[k] - nn.learning_rate*nn.W_grad[k]/(np.sqrt(nn.rW[k]) + 0.001)\n",
    "                nn.b[k] = nn.b[k] - nn.learning_rate*nn.b_grad[k]/(np.sqrt(nn.rb[k]) + 0.001)\n",
    "                \n",
    "            elif method == 'Momentum':\n",
    "                rho = 0.1 #rho = 0.1\n",
    "                nn.vW[k] = rho * nn.vW[k] - nn.learning_rate*nn.W_grad[k]\n",
    "                nn.vb[k] = rho * nn.vb[k] - nn.learning_rate*nn.b_grad[k]\n",
    "                nn.W[k] = nn.W[k] + nn.vW[k]\n",
    "                nn.b[k] = nn.b[k] + nn.vb[k]\n",
    "\n",
    "            elif method == 'RMSProp':\n",
    "                rho = 0.9 #rho = 0.9\n",
    "                nn.rW[k] = rho * nn.rW[k] + 0.1*nn.W_grad[k]**2\n",
    "                nn.rb[k] = rho * nn.rb[k] + 0.1*nn.b_grad[k]**2\n",
    "\n",
    "                nn.W[k] = nn.W[k] - nn.learning_rate*nn.W_grad[k]/(np.sqrt(nn.rW[k]) + 0.001)\n",
    "                nn.b[k] = nn.b[k] - nn.learning_rate*nn.b_grad[k]/(np.sqrt(nn.rb[k]) + 0.001) #rho = 0.9\n",
    "\n",
    "            elif method == 'Adam':\n",
    "                rho1 = 0.9\n",
    "                rho2 = 0.999\n",
    "                nn.sW[k] = 0.9*nn.sW[k] + 0.1*nn.W_grad[k]\n",
    "                nn.sb[k] = 0.9*nn.sb[k] + 0.1*nn.b_grad[k]\n",
    "                nn.rW[k] = 0.999*nn.rW[k] + 0.001*nn.W_grad[k]**2\n",
    "                nn.rb[k] = 0.999*nn.rb[k] + 0.001*nn.b_grad[k]**2\n",
    "\n",
    "                newS = nn.sW[k] / (1 - rho1)\n",
    "                newR = nn.rW[k] / (1 - rho2)\n",
    "                nn.W[k] = nn.W[k] - nn.learning_rate*newS/np.sqrt(newR + 0.00001)\n",
    "                newS = nn.sb[k] / (1 - rho1)\n",
    "                newR = nn.rb[k] / (1 - rho2)\n",
    "                nn.b[k] = nn.b[k] -nn.learning_rate*newS/np.sqrt(newR + 0.00001)#rho1 = 0.9, rho2 = 0.999, delta = 0.00001\n",
    "\n",
    "        else:\n",
    "            if method == 'normal':\n",
    "                nn.W[k] = nn.W[k] - nn.learning_rate*nn.W_grad[k]\n",
    "                nn.b[k] = nn.b[k] - nn.learning_rate*nn.b_grad[k]\n",
    "                nn.Gamma[k] = nn.Gamma[k] - nn.learning_rate*nn.Gamma_grad[k]\n",
    "                nn.Beta[k] = nn.Beta[k] - nn.learning_rate*nn.Beta_grad[k]\n",
    "                \n",
    "            elif method == 'AdaGrad':\n",
    "                nn.rW[k] = nn.rW[k] + nn.W_grad[k]**2\n",
    "                nn.rb[k] = nn.rb[k] + nn.b_grad[k]**2\n",
    "                nn.rGamma[k] = nn.rGamma[k] + nn.Gamma_grad[k]**2\n",
    "                nn.rBeta[k] = nn.rBeta[k] + nn.Beta_grad[k]**2\n",
    "                nn.W[k] = nn.W[k] - nn.learning_rate*nn.W_grad[k]/(np.sqrt(nn.rW[k]) + 0.001)\n",
    "                nn.b[k] = nn.b[k] - nn.learning_rate*nn.b_grad[k]/(np.sqrt(nn.rb[k]) + 0.001)\n",
    "                nn.Gamma[k] = nn.Gamma[k] - nn.learning_rate*nn.Gamma_grad[k] / (np.sqrt(nn.rGamma[k]) + 0.001)\n",
    "                nn.Beta[k] = nn.Beta[k] - nn.learning_rate*nn.Beta_grad[k] / (np.sqrt(nn.rBeta[k]) + 0.001)\n",
    "                \n",
    "            elif method == 'RMSProp':\n",
    "                nn.rW[k] = 0.9*nn.rW[k] + 0.1*nn.W_grad[k]**2\n",
    "                nn.rb[k] = 0.9*nn.rb[k] + 0.1*nn.b_grad[k]**2\n",
    "                nn.rGamma[k] = 0.9*nn.rGamma[k] + 0.1*nn.Gamma_grad[k]**2\n",
    "                nn.rBeta[k] = 0.9*nn.rBeta[k] + 0.1*nn.Beta_grad[k]**2\n",
    "                nn.W[k] = nn.W[k] - nn.learning_rate*nn.W_grad[k]/(np.sqrt(nn.rW[k]) + 0.001)\n",
    "                nn.b[k] = nn.b[k] - nn.learning_rate*nn.b_grad[k]/(np.sqrt(nn.rb[k]) + 0.001)\n",
    "                nn.Gamma[k] = nn.Gamma[k] - nn.learning_rate*nn.Gamma_grad[k] / (np.sqrt(nn.rGamma[k]) + 0.001)\n",
    "                nn.Beta[k] = nn.Beta[k] - nn.learning_rate*nn.Beta_grad[k] / (np.sqrt(nn.rBeta[k]) + 0.001) #rho = 0.9\n",
    "\n",
    "            elif method == 'Momentum':\n",
    "                rho = 0.1 #rho = 0.1\n",
    "                nn.vW[k] = rho * nn.vW[k] - nn.learning_rate*nn.W_grad[k]\n",
    "                nn.vb[k] = rho * nn.vb[k] - nn.learning_rate*nn.b_grad[k]\n",
    "                nn.vGamma[k] = rho * nn.vGamma[k] - nn.learning_rate*nn.Gamma_grad[k]\n",
    "                nn.vBeta[k] = rho * nn.vBeta[k] - nn.learning_rate*nn.Beta_grad[k]\n",
    "                nn.W[k] = nn.W[k] + nn.vW[k]\n",
    "                nn.b[k] = nn.b[k] + nn.vb[k]\n",
    "                nn.Gamma[k] = nn.Gamma[k] + nn.vGamma[k]\n",
    "                nn.Beta[k] = nn.Beta[k] + nn.vBeta[k]\n",
    "\n",
    "            elif method == 'Adam':\n",
    "                nn.sW[k] = 0.9*nn.sW[k] + 0.1*nn.W_grad[k]\n",
    "                nn.sb[k] = 0.9*nn.sb[k] + 0.1*nn.b_grad[k]\n",
    "                nn.sGamma[k] = 0.9*nn.sGamma[k] + 0.1*nn.Gamma_grad[k]\n",
    "                nn.sBeta[k] = 0.9*nn.sBeta[k] + 0.1*nn.Beta_grad[k]\n",
    "                nn.rW[k] = 0.999*nn.rW[k] + 0.001*nn.W_grad[k]**2\n",
    "                nn.rb[k] = 0.999*nn.rb[k] + 0.001*nn.b_grad[k]**2\n",
    "                nn.rBeta[k] = 0.999*nn.rBeta[k] + 0.001*nn.Beta_grad[k]**2\n",
    "                nn.rGamma[k] = 0.999*nn.rGamma[k] + 0.001*nn.Gamma_grad[k]**2\n",
    "                nn.W[k] = nn.W[k] -10 * nn.learning_rate*nn.sW[k]/np.sqrt(1000 * nn.rW[k]+0.00001)\n",
    "                nn.b[k] = nn.b[k] -10 * nn.learning_rate*nn.sb[k]/np.sqrt(1000 * nn.rb[k]+0.00001)\n",
    "                nn.Gamma[k] = nn.Gamma[k] -10 * nn.learning_rate*nn.sGamma[k]/np.sqrt(1000 * nn.rGamma[k]+0.00001)\n",
    "                nn.Beta[k] = nn.Beta[k] -10 * nn.learning_rate*nn.sBeta[k]/np.sqrt(1000 * nn.rBeta[k]+0.00001) #rho1 = 0.9, rho2 = 0.999, delta = 0.00001\n",
    "\n",
    "    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d1b7f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nn_forward.py\n",
    "def nn_forward(nn, batch_x, batch_y):\n",
    "    s = len(nn.cost) + 1 \n",
    "    batch_x = batch_x.T \n",
    "    batch_y = batch_y.T \n",
    "    m = batch_x.shape[1]\n",
    "    nn.a[0] = batch_x \n",
    "\n",
    "    cost2 = 0 \n",
    "    for k in range(1, nn.depth):\n",
    "        y = np.dot(nn.W[k-1], nn.a[k-1]) + np.tile(nn.b[k-1], (1, m)) #np.tile就是matlab中的repmat(replicate matrix)\n",
    "        \n",
    "        if nn.batch_normalization:\n",
    "            nn.E[k-1] = nn.E[k-1]*nn.vecNum + np.array([np.sum(y, axis=1)]).T\n",
    "            nn.S[k-1] = nn.S[k-1]**2 * (nn.vecNum - 1) + np.array([(m - 1)*np.std(y,ddof=1,axis=1)** 2]).T #ddof=1计算无偏估计\n",
    "            nn.vecNum = nn.vecNum + m \n",
    "            nn.E[k-1] = nn.E[k-1] / nn.vecNum \n",
    "            nn.S[k-1] = np.sqrt(nn.S[k-1] / (nn.vecNum - 1)) \n",
    "            y = (y - np.tile(nn.E[k-1], (1, m))) / np.tile(nn.S[k-1]+0.0001*np.ones(nn.S[k-1].shape), (1, m)) \n",
    "            y = nn.Gamma[k-1]*y + nn.Beta[k-1] \n",
    "\n",
    "        if k == nn.depth - 1:\n",
    "            f = nn.output_function\n",
    "            if f == 'sigmoid' :\n",
    "                nn.a[k] = sigmoid(y)\n",
    "            elif f == 'tanh' :\n",
    "                nn.a[k] = np.tanh(y)\n",
    "            elif f == 'relu' :\n",
    "                nn.a[k] = np.maximum(y, 0)\n",
    "            elif f == 'softmax' :\n",
    "                nn.a[k] = softmax(y)\n",
    "\n",
    "        else:\n",
    "            f = nn.active_function\n",
    "            if f == 'sigmoid' :\n",
    "                nn.a[k] = sigmoid(y)\n",
    "            elif f == 'tanh' :\n",
    "                nn.a[k] = np.tanh(y)\n",
    "            elif f == 'relu' :\n",
    "                nn.a[k] = np.maximum(y, 0)\n",
    "\n",
    "        cost2 = cost2 + np.sum(nn.W[k-1]**2)\n",
    "\n",
    "    if nn.encoder == 1:\n",
    "        roj = np.sum(nn.a[2], axis=1) / m \n",
    "        nn.cost[s] = 0.5 * np.sum((nn.a[k] -batch_y)**2) / m + 0.5 * nn.weight_decay * cost2 + 3 * sum(nn.sparsity * np.log(nn.sparsity / roj) + (1-nn.sparsity) * np.log((1-nn.sparsity) / (1-roj)))\n",
    "    else:\n",
    "        if nn.objective_function == 'MSE':\n",
    "            nn.cost[s] = 0.5 / m * sum(sum((nn.a[k] -batch_y)** 2)) + 0.5 * nn.weight_decay * cost2 \n",
    "        elif nn.objective_function == 'Cross Entropy':\n",
    "            nn.cost[s] = -0.5 * sum(sum(batch_y*np.log(nn.a[k]))) / m + 0.5 * nn.weight_decay * cost2 \n",
    "    # nn.cost[s]\n",
    "    \n",
    "    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1c414b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nn_backpropagation.py\n",
    "\n",
    "def nn_backpropagation(nn, batch_y) :\n",
    "    batch_y = batch_y.T\n",
    "    m = nn.a[0].shape[1]\n",
    "    nn.theta[1] = 0\n",
    "    f = nn.output_function\n",
    "    if f == 'sigmoid' :\n",
    "        nn.theta[nn.depth-1] = -(batch_y - nn.a[nn.depth-1]) * nn.a[nn.depth-1] * (1 - nn.a[nn.depth-1])\n",
    "    if f == 'tanh' :\n",
    "        nn.theta[nn.depth-1] = -(batch_y - nn.a[nn.depth-1]) * (1 - nn.a[nn.depth-1]**2)\n",
    "    if f == 'softmax' :\n",
    "        y = np.dot(nn.W[nn.depth - 2], nn.a[nn.depth - 2]) + np.tile(nn.b[nn.depth - 2], (1, m))\n",
    "        nn.theta[nn.depth-1] = nn.a[nn.depth-1] - batch_y\n",
    "\n",
    "    if nn.batch_normalization :\n",
    "        x = np.dot(nn.W[nn.depth - 2], nn.a[nn.depth -2]) + np.tile(nn.b[nn.depth - 2], (1, m))\n",
    "        x = (x - np.tile(nn.E[nn.depth -2], (1, m))) / np.tile(nn.S[nn.depth -2] + 0.0001*np.ones(nn.S[nn.depth - 2].shape), (1, m))\n",
    "        temp = nn.theta[nn.depth-1] * x\n",
    "        nn.Gamma_grad[nn.depth - 2] = sum(np.mean(temp, axis = 1))\n",
    "        nn.Beta_grad[nn.depth - 2] = sum(np.mean(nn.theta[nn.depth-1], axis = 1))\n",
    "        nn.theta[nn.depth - 1] = nn.Gamma[nn.depth - 2]*nn.theta[nn.depth-1] / np.tile((nn.S[nn.depth - 2] + 0.0001), (1, m))\n",
    "\n",
    "    nn.W_grad[nn.depth - 2] = np.dot(nn.theta[nn.depth-1], nn.a[nn.depth - 2].T) / m + nn.weight_decay*nn.W[nn.depth - 2]\n",
    "    nn.b_grad[nn.depth - 2] = np.array([np.sum(nn.theta[nn.depth-1], axis=1) / m]).T \n",
    "    #因为np.sum()返回维度为(n,)，会让之后的加法操作错误，所以要转换为(n,1)维度矩阵，下面的也是一样\n",
    "    \n",
    "    f = nn.active_function\n",
    "    if f == 'sigmoid':\n",
    "        if nn.encoder == 0 :\n",
    "            for ll in range(1, nn.depth - 1) :\n",
    "                k = nn.depth - ll-1\n",
    "                nn.theta[k] = np.dot(nn.W[k].T, nn.theta[k + 1])*nn.a[k]* (1 - nn.a[k])\n",
    "                if nn.batch_normalization :\n",
    "                    x = np.dot(nn.W[k - 1], nn.a[k - 1]) + np.tile(nn.b[k - 1], (1, m))\n",
    "                    x = (x - np.tile(nn.E[k - 1], (1, m))) / np.tile(nn.S[k - 1] + 0.0001*np.ones(nn.S[k - 1].shape), (1, m))\n",
    "                    temp = nn.theta[k]*x\n",
    "                    nn.Gamma_grad[k - 1] = sum(np.mean(temp, axis = 1))\n",
    "                    nn.Beta_grad[k - 1] = sum(np.mean(nn.theta[k], axis = 1))\n",
    "                    nn.theta[k] = (nn.Gamma[k - 1]* nn.theta[k]) / np.tile((nn.S[k - 1] + 0.0001), (1, m))\n",
    "                    pass\n",
    "\n",
    "                nn.W_grad[k - 1] = np.dot(nn.theta[k], nn.a[k - 1].T) / m + nn.weight_decay*nn.W[k - 1]\n",
    "                nn.b_grad[k - 1] = np.array([np.sum(nn.theta[k], axis = 1) / m]).T\n",
    "\n",
    "        else:\n",
    "            #encoder完全按照matlab的NN，但貌似是有错误的，用encoder会报错，因为theta[2]（对应matlab的theta{3}）没有赋值\n",
    "            roj = np.array([np.sum(nn.a[1], axis = 1) / m]).T \n",
    "            temp = (-nn.sparsity / roj + (1 - nn.sparsity) / (1 - roj))\n",
    "            nn.theta[1] = (np.dot(nn.W[1].T, nn.theta[2]) + nn.beta*repmat(temp, 1, m))*M\n",
    "            nn.W_grad[0] = np.dot(nn.theta[1], nn.a[0].T) / m + nn.weight_decay*nn.W[0]\n",
    "            nn.b_grad[0] = np.array([np.sum(nn.theta[1], axis = 1) / m]).T\n",
    "            \n",
    "\n",
    "    elif f == 'tanh':\n",
    "        for ll in range(1, nn.depth-1) :\n",
    "            if nn.encoder == 0 :\n",
    "                k = nn.depth - ll-1 \n",
    "                nn.theta[k] = np.dot(nn.W[k].T,nn.theta[k + 1])*(1 - nn.a[k]**2)\n",
    "                if nn.batch_normalization :\n",
    "                    x = np.dot(nn.W[k - 1], nn.a[k - 1]) + np.tile(nn.b[k - 1], (1, m))\n",
    "                    x = (x - np.tile(nn.E[k - 1], (1, m))) / np.tile(nn.S[k - 1] + 0.0001*np.ones(nn.S[k - 1].shape), (1, m))\n",
    "                    temp = nn.theta[k]*x\n",
    "                    nn.Gamma_grad[k - 1] = sum(np.mean(temp, axis = 1))\n",
    "                    nn.Beta_grad[k - 1] = sum(np.mean(nn.theta[k], axis = 1))\n",
    "                    nn.theta[k] = (nn.Gamma[k - 1]* nn.theta[k]) / np.tile((nn.S[k - 1] + 0.0001), (1, m))\n",
    "                    pass\n",
    "\n",
    "                nn.W_grad[k - 1] = np.dot(nn.theta[k], nn.a[k - 1].T) / m + nn.weight_decay*nn.W[k - 1]\n",
    "                nn.b_grad[k - 1] = np.array([np.sum(nn.theta[k], axis = 1) / m]).T\n",
    "\n",
    "            else:\n",
    "                roj = np.array([np.sum(nn.a[1], axis = 1) / m]).T\n",
    "                temp = (-nn.sparsity / roj + (1 - nn.sparsity) / (1 - roj))\n",
    "                nn.theta[1] = (np.dot(nn.W[1].T, nn.theta[2]) + nn.beta*repmat(temp, 1, m))*M\n",
    "                nn.W_grad[0] = np.dot(nn.theta[1], nn.a[0].T) / m + nn.weight_decay*nn.W[0]\n",
    "                nn.b_grad[0] = np.array([np.sum(nn.theta[1], axis = 1) / m]).T\n",
    "\n",
    "    elif f == 'relu':\n",
    "        if nn.encoder == 0 :\n",
    "            for ll in range(1, nn.depth - 1) :\n",
    "                k = nn.depth - ll-1\n",
    "                nn.theta[k] = np.dot(nn.W[k].T,nn.theta[k + 1])* (nn.a[k] > 0)\n",
    "                if nn.batch_normalization :\n",
    "                    x = np.dot(nn.W[k - 1], nn.a[k - 1]) + np.tile(nn.b[k - 1], (1, m))\n",
    "                    x = (x - np.tile(nn.E[k - 1], (1, m))) / np.tile(nn.S[k - 1] + 0.0001*np.ones(nn.S[k - 1].shape), (1, m))\n",
    "                    temp = nn.theta[k]*x\n",
    "                    nn.Gamma_grad[k - 1] = sum(np.mean(temp, axis = 1))\n",
    "                    nn.Beta_grad[k - 1] = sum(np.mean(nn.theta[k], axis = 1))\n",
    "                    nn.theta[k] = (nn.Gamma[k - 1]* nn.theta[k]) / np.tile((nn.S[k - 1] + 0.0001), (1, m))\n",
    "                    pass\n",
    "\n",
    "                nn.W_grad[k - 1] = np.dot(nn.theta[k], nn.a[k - 1].T) / m + nn.weight_decay*nn.W[k - 1]\n",
    "                nn.b_grad[k - 1] = np.array([np.sum(nn.theta[k], axis = 1) / m]).T\n",
    "\n",
    "        else:\n",
    "            roj = np.array([np.sum(nn.a[1], axis = 1) / m]).T\n",
    "            temp = (-nn.sparsity / roj + (1 - nn.sparsity) / (1 - roj))\n",
    "            M = np.maximum(nn.a[1], 0)\n",
    "            M = M / np.maximum(M, 0.001)\n",
    "\n",
    "            nn.theta[1] = (np.dot(nn.W[1].T, nn.theta[2]) + nn.beta*repmat(temp, 1, m))*M\n",
    "            nn.W_grad[0] = np.dot(nn.theta[1], nn.a[0].T) / m + nn.weight_decay*nn.W[0]\n",
    "            nn.b_grad[0] = np.array([np.sum(nn.theta[1], axis = 1) / m]).T\n",
    "    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56444132",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nn_predict.py\n",
    "\n",
    "def nn_predict(nn, batch_x):\n",
    "    batch_x = batch_x.T \n",
    "    m = batch_x.shape[1]\n",
    "    nn.a[0] = batch_x \n",
    "    for k in range(1, nn.depth):\n",
    "        y = np.dot(nn.W[k-1], nn.a[k-1]) + np.tile(nn.b[k-1], (1, m))\n",
    "        if nn.batch_normalization:\n",
    "            y = (y - np.tile(nn.E[k-1], (1, m))) / np.tile(nn.S[k-1]+0.0001*np.ones(nn.S[k-1].shape), (1, m)) \n",
    "            y = nn.Gamma[k-1]*y + nn.Beta[k-1] \n",
    "\n",
    "        if k == nn.depth-1:\n",
    "            f = nn.output_function\n",
    "            if f == 'sigmoid':\n",
    "                nn.a[k] = sigmoid(y) \n",
    "            elif f == 'tanh':\n",
    "                nn.a[k] = np.tanh(y) \n",
    "            elif f == 'relu':\n",
    "                nn.a[k] = np.maximum(y, 0) \n",
    "            elif f == 'softmax':\n",
    "                nn.a[k] = softmax(y) \n",
    "\n",
    "        else:\n",
    "            f = nn.active_function\n",
    "            if f == 'sigmoid':\n",
    "                nn.a[k] = sigmoid(y) \n",
    "            elif f == 'tanh':\n",
    "                nn.a[k] = np.tanh(y) \n",
    "            elif f == 'relu':\n",
    "                nn.a[k] = np.maximum(y, 0) \n",
    "            \n",
    "    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7403ef4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nn_train.py\n",
    "\n",
    "def nn_train(nn,option,train_x,train_y):\n",
    "    iteration = option.iteration\n",
    "    batch_size = option.batch_size\n",
    "    m = train_x.shape[0]\n",
    "    num_batches = m / batch_size\n",
    "    for k in range(iteration): \n",
    "        kk = np.random.permutation(m)\n",
    "        for l in range(int(num_batches)):\n",
    "            batch_x = train_x[kk[l * batch_size : (l + 1) * batch_size], :] #(l+1)*batch_size也可以改成max((l+1)*batch_size, len(kk))\n",
    "            batch_y = train_y[kk[l * batch_size : (l + 1) * batch_size], :]\n",
    "            nn = nn_forward(nn,batch_x,batch_y)\n",
    "            nn = nn_backpropagation(nn,batch_y)\n",
    "            nn = nn_applygradient(nn)\n",
    "            \n",
    "    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "527a5bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nn_test.py\n",
    "\n",
    "def nn_test(nn,test_x,test_y):\n",
    "    nn = nn_predict(nn,test_x)\n",
    "    y_output = nn.a[nn.depth-1]\n",
    "    y_output = y_output.T\n",
    "    label = np.argmax(y_output, axis=1) #按行找出最大元素所在下标\n",
    "    expectation = np.argmax(test_y, axis=1)\n",
    "    wrongs = sum(label != expectation) #求预测与期望不相等的个数\n",
    "    success_ratio = 1-wrongs/test_y.shape[0]\n",
    "    \n",
    "    return wrongs, success_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c38c250",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from matplotlib.pyplot import *\n",
    "\n",
    "def nn_testChess():\n",
    "    with open('krkopt.data') as my_data:  # 读取文件部分\n",
    "        lines = my_data.readlines()\n",
    "        data = np.zeros((28056, 6), dtype=float)\n",
    "        label = np.zeros((28056, 2), dtype=float)\n",
    "        i = 0\n",
    "        for line in lines:\n",
    "            line = line.split(',')  # 以逗号分开\n",
    "            if i == 0:\n",
    "                line[0] = 'a'  # 不知道为什么第一个数据乱码，用写字板打开是'a'\n",
    "\n",
    "            line[0] = ord(line[0]) - 96\n",
    "            line[1] = float(line[1]) - 48\n",
    "            line[2] = ord(line[2]) - 96\n",
    "            line[3] = float(line[3]) - 48\n",
    "            line[4] = ord(line[4]) - 96\n",
    "            line[5] = float(line[5]) - 48\n",
    "            data[i, :] = line[:-1]\n",
    "\n",
    "            if line[6][0] == 'd':\n",
    "                label[i] = np.array([1, 0])\n",
    "            else:\n",
    "                label[i] = np.array([0, 1])\n",
    "            i += 1\n",
    "            if i == 28056:\n",
    "                break\n",
    "\n",
    "    ratioTraining = 0.4\n",
    "    ratioValidation = 0.1\n",
    "    ratioTesting = 0.5\n",
    "    xTraining, xTesting, yTraining, yTesting = train_test_split(data, label, test_size=1 - ratioTraining,\n",
    "                                                                random_state=0)  # 随机分配数据集\n",
    "    xTesting, xValidation, yTesting, yValidation = train_test_split(xTesting, yTesting,\n",
    "                                                                    test_size=ratioValidation / ratioTesting,\n",
    "                                                                    random_state=0)\n",
    "    # 拆分成测试集和验证集\n",
    "\n",
    "    scaler = StandardScaler(copy=False)\n",
    "    scaler.fit(xTraining)\n",
    "    scaler.transform(xTraining)  # 标准归一化\n",
    "    scaler.transform(xTesting)\n",
    "    scaler.transform(xValidation)\n",
    "\n",
    "    nn = NN(layer=[6, 20, 20, 20, 2], active_function='relu', learning_rate=0.01, batch_normalization=1,\n",
    "            optimization_method='Adam',\n",
    "            objective_function='Cross Entropy')\n",
    "\n",
    "    option = Option()\n",
    "    option.batch_size = 50\n",
    "    option.iteration = 1\n",
    "\n",
    "    iteration = 0\n",
    "    maxAccuracy = 0\n",
    "    totalAccuracy = []\n",
    "    totalCost = []\n",
    "    maxIteration = 20\n",
    "    while iteration < maxIteration:\n",
    "        iteration = iteration + 1\n",
    "        nn = nn_train(nn, option, xTraining, yTraining)\n",
    "        totalCost.append(sum(nn.cost.values()) / len(nn.cost.values()))\n",
    "        # plot(totalCost)\n",
    "        (wrongs, accuracy) = nn_test(nn, xValidation, yValidation)\n",
    "        totalAccuracy.append(accuracy)\n",
    "        if accuracy > maxAccuracy:\n",
    "            maxAccuracy = accuracy\n",
    "            storedNN = nn\n",
    "\n",
    "        cost = totalCost[iteration - 1]\n",
    "        print(accuracy)\n",
    "        print(totalCost[iteration - 1])\n",
    "\n",
    "    subplot(2, 1, 1)\n",
    "    plot(totalCost, color='red')\n",
    "    title('Average Objective Function Value on the Training Set')\n",
    "\n",
    "    subplot(2, 1, 2)\n",
    "    plot(totalAccuracy, color='red')\n",
    "    ylim([0.8, 1])\n",
    "    title('Accuracy on the Validation Set')\n",
    "    tight_layout(2)\n",
    "    show()\n",
    "\n",
    "    wrongs, accuracy = nn_test(storedNN, xTesting, yTesting)\n",
    "    print('acc:', accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "685dce62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9664389664389664\n",
      "0.11221213245131223\n",
      "0.9934659934659935\n",
      "0.06987707183838372\n",
      "0.9937629937629937\n",
      "0.05265935120050501\n",
      "0.9946539946539946\n",
      "0.04306624606082997\n",
      "0.9946539946539946\n",
      "0.03687409614193872\n",
      "0.994059994059994\n",
      "0.03251824275208296\n",
      "0.9952479952479952\n",
      "0.029221841552463493\n",
      "0.9943569943569943\n",
      "0.02661989552649125\n",
      "0.994059994059994\n",
      "0.024557498981575922\n",
      "0.9958419958419958\n",
      "0.022799409439782668\n",
      "0.9946539946539946\n",
      "0.021324239093081128\n",
      "0.9931689931689932\n",
      "0.020071651196202134\n",
      "0.9949509949509949\n",
      "0.018992656698056058\n",
      "0.9946539946539946\n",
      "0.018034763429927624\n",
      "0.9964359964359965\n",
      "0.01720508364783913\n",
      "0.9961389961389961\n",
      "0.016435860682391775\n",
      "0.9955449955449955\n",
      "0.01575862078779709\n",
      "0.9961389961389961\n",
      "0.015145801362436638\n",
      "0.9970299970299971\n",
      "0.01459997157800423\n",
      "0.9931689931689932\n",
      "0.014093859497147468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-12-f0b85e1ade3c>:84: MatplotlibDeprecationWarning: Passing the pad parameter of tight_layout() positionally is deprecated since Matplotlib 3.3; the parameter will become keyword-only two minor releases later.\n",
      "  tight_layout(2)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.9932427415162991\n"
     ]
    }
   ],
   "source": [
    "nn_testChess()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
